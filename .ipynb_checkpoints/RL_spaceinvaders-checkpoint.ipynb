{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67381f35-7f4c-40e6-a37f-04cbaa8f59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2bd055-1b32-425e-8473-0ad4b03fdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603b6d38-e28d-4a79-a7cb-e588c125ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environmnent and preprocess\n",
    "def instantiate_environmnent(game=\"SpaceInvaders-v4\"):\n",
    "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "    env = AtariPreprocessing(env, grayscale_newaxis=False, frame_skip=4)\n",
    "    env = FrameStack(env, 4)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f2dc7b-069b-4583-b453-d07f7669325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 6\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer = layers.Conv2D(32, 8, activation=\"relu\")(inputs)\n",
    "    layer = layers.MaxPool2D(pool_size=(4, 4))(layer)\n",
    "    layer = layers.Conv2D(64, 4, activation=\"relu\")(layer)\n",
    "    layer = layers.MaxPool2D(pool_size=(2, 2))(layer)\n",
    "    layer = layers.Conv2D(64, 3, activation=\"relu\")(layer)\n",
    "\n",
    "    layer = layers.Flatten()(layer)\n",
    "\n",
    "    layer = layers.Dense(512, activation=\"relu\")(layer)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0422886e-9d9c-4e28-b5de-9700e721279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(reward):\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f72388b-9cd3-497a-a40e-80e012126530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "----------\n",
      "Game Number: 1\n",
      "EXPLORATION\n",
      "Score: 90.0\n",
      "Reward: 90.0\n",
      "Timesteps: 474\n",
      "Game Frames Survived: 1917\n",
      "Epsilon: 0.9573399999999836\n",
      "Running Score (last 10 games): 90.0\n",
      "Running Reward (last 10 games): 90.0\n",
      "Explored: 1, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 2\n",
      "EXPLORATION\n",
      "Score: 15.0\n",
      "Reward: 15.0\n",
      "Timesteps: 301\n",
      "Game Frames Survived: 1231\n",
      "Epsilon: 0.9302499999999733\n",
      "Running Score (last 10 games): 15.0\n",
      "Running Reward (last 10 games): 52.5\n",
      "Explored: 2, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 3\n",
      "EXPLORATION\n",
      "Score: 210.0\n",
      "Reward: 210.0\n",
      "Timesteps: 1069\n",
      "Game Frames Survived: 4295\n",
      "Epsilon: 0.8340399999999364\n",
      "Running Score (last 10 games): 210.0\n",
      "Running Reward (last 10 games): 105.0\n",
      "Explored: 3, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 4\n",
      "EXPLORATION\n",
      "Score: 410.0\n",
      "Reward: 410.0\n",
      "Timesteps: 786\n",
      "Game Frames Survived: 3163\n",
      "Epsilon: 0.7632999999999093\n",
      "Running Score (last 10 games): 410.0\n",
      "Running Reward (last 10 games): 181.25\n",
      "Explored: 4, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 5\n",
      "EXPLORATION\n",
      "Score: 630.0\n",
      "Reward: 630.0\n",
      "Timesteps: 1032\n",
      "Game Frames Survived: 4145\n",
      "Epsilon: 0.6704199999998737\n",
      "Running Score (last 10 games): 630.0\n",
      "Running Reward (last 10 games): 271.0\n",
      "Explored: 5, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 6\n",
      "EXPLORATION\n",
      "Score: 115.0\n",
      "Reward: 115.0\n",
      "Timesteps: 550\n",
      "Game Frames Survived: 2217\n",
      "Epsilon: 0.6209199999998547\n",
      "Running Score (last 10 games): 115.0\n",
      "Running Reward (last 10 games): 245.0\n",
      "Explored: 6, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 7\n",
      "EXPLORATION\n",
      "Score: 75.0\n",
      "Reward: 75.0\n",
      "Timesteps: 388\n",
      "Game Frames Survived: 1561\n",
      "Epsilon: 0.5859999999998413\n",
      "Running Score (last 10 games): 75.0\n",
      "Running Reward (last 10 games): 220.71428571428572\n",
      "Explored: 7, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 8\n",
      "EXPLORATION\n",
      "Score: 230.0\n",
      "Reward: 230.0\n",
      "Timesteps: 743\n",
      "Game Frames Survived: 2991\n",
      "Epsilon: 0.5191299999998157\n",
      "Running Score (last 10 games): 230.0\n",
      "Running Reward (last 10 games): 221.875\n",
      "Explored: 8, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 9\n",
      "EXPLORATION\n",
      "Score: 90.0\n",
      "Reward: 90.0\n",
      "Timesteps: 383\n",
      "Game Frames Survived: 1545\n",
      "Epsilon: 0.48465999999981196\n",
      "Running Score (last 10 games): 90.0\n",
      "Running Reward (last 10 games): 207.22222222222223\n",
      "Explored: 9, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 10\n",
      "EXPLORATION\n",
      "Score: 270.0\n",
      "Reward: 270.0\n",
      "Timesteps: 617\n",
      "Game Frames Survived: 2473\n",
      "Epsilon: 0.42912999999982493\n",
      "Running Score (last 10 games): 270.0\n",
      "Running Reward (last 10 games): 213.5\n",
      "Explored: 10, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 11\n",
      "EXPLORATION\n",
      "Score: 225.0\n",
      "Reward: 225.0\n",
      "Timesteps: 597\n",
      "Game Frames Survived: 2387\n",
      "Epsilon: 0.3753999999998375\n",
      "Running Score (last 10 games): 225.0\n",
      "Running Reward (last 10 games): 214.54545454545453\n",
      "Explored: 11, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 12\n",
      "EXPLORATION\n",
      "Score: 80.0\n",
      "Reward: 80.0\n",
      "Timesteps: 285\n",
      "Game Frames Survived: 1159\n",
      "Epsilon: 0.34974999999984346\n",
      "Running Score (last 10 games): 80.0\n",
      "Running Reward (last 10 games): 203.33333333333334\n",
      "Explored: 12, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 13\n",
      "EXPLORATION\n",
      "Score: 180.0\n",
      "Reward: 180.0\n",
      "Timesteps: 549\n",
      "Game Frames Survived: 2217\n",
      "Epsilon: 0.300339999999855\n",
      "Running Score (last 10 games): 180.0\n",
      "Running Reward (last 10 games): 201.53846153846155\n",
      "Explored: 13, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 14\n",
      "EXPLORATION\n",
      "Score: 165.0\n",
      "Reward: 165.0\n",
      "Timesteps: 423\n",
      "Game Frames Survived: 1711\n",
      "Epsilon: 0.2622699999998639\n",
      "Running Score (last 10 games): 165.0\n",
      "Running Reward (last 10 games): 198.92857142857142\n",
      "Explored: 14, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 15\n",
      "EXPLORATION\n",
      "Score: 30.0\n",
      "Reward: 30.0\n",
      "Timesteps: 365\n",
      "Game Frames Survived: 1477\n",
      "Epsilon: 0.2294199999998652\n",
      "Running Score (last 10 games): 30.0\n",
      "Running Reward (last 10 games): 187.66666666666666\n",
      "Explored: 15, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 16\n",
      "EXPLORATION\n",
      "Score: 240.0\n",
      "Reward: 240.0\n",
      "Timesteps: 729\n",
      "Game Frames Survived: 2929\n",
      "Epsilon: 0.1638099999998603\n",
      "Running Score (last 10 games): 240.0\n",
      "Running Reward (last 10 games): 190.9375\n",
      "Explored: 16, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 17\n",
      "EXPLORATION\n",
      "Score: 155.0\n",
      "Reward: 155.0\n",
      "Timesteps: 609\n",
      "Game Frames Survived: 2453\n",
      "Epsilon: 0.10899999999985618\n",
      "Running Score (last 10 games): 155.0\n",
      "Running Reward (last 10 games): 188.8235294117647\n",
      "Explored: 17, Exploited: 0\n",
      "running reward: 188.82 at episode 17, frame count 10000\n",
      "----------\n",
      "----------\n",
      "Game Number: 18\n",
      "EXPLORATION\n",
      "Score: 45.0\n",
      "Reward: 45.0\n",
      "Timesteps: 270\n",
      "Game Frames Survived: 1091\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 45.0\n",
      "Running Reward (last 10 games): 180.83333333333334\n",
      "Explored: 18, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 19\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 515\n",
      "Game Frames Survived: 2085\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 110.0\n",
      "Running Reward (last 10 games): 177.10526315789474\n",
      "Explored: 19, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 20\n",
      "EXPLORATION\n",
      "Score: 210.0\n",
      "Reward: 210.0\n",
      "Timesteps: 586\n",
      "Game Frames Survived: 2363\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 210.0\n",
      "Running Reward (last 10 games): 178.75\n",
      "Explored: 20, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 21\n",
      "EXPLORATION\n",
      "Score: 255.0\n",
      "Reward: 255.0\n",
      "Timesteps: 777\n",
      "Game Frames Survived: 3133\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 255.0\n",
      "Running Reward (last 10 games): 182.38095238095238\n",
      "Explored: 21, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 22\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 542\n",
      "Game Frames Survived: 2187\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 110.0\n",
      "Running Reward (last 10 games): 179.0909090909091\n",
      "Explored: 22, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 23\n",
      "EXPLORATION\n",
      "Score: 50.0\n",
      "Reward: 50.0\n",
      "Timesteps: 303\n",
      "Game Frames Survived: 1237\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 50.0\n",
      "Running Reward (last 10 games): 173.47826086956522\n",
      "Explored: 23, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 24\n",
      "EXPLORATION\n",
      "Score: 95.0\n",
      "Reward: 95.0\n",
      "Timesteps: 396\n",
      "Game Frames Survived: 1601\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 95.0\n",
      "Running Reward (last 10 games): 170.20833333333334\n",
      "Explored: 24, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 25\n",
      "EXPLORATION\n",
      "Score: 105.0\n",
      "Reward: 105.0\n",
      "Timesteps: 480\n",
      "Game Frames Survived: 1945\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 105.0\n",
      "Running Reward (last 10 games): 167.6\n",
      "Explored: 25, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 26\n",
      "EXPLORATION\n",
      "Score: 45.0\n",
      "Reward: 45.0\n",
      "Timesteps: 290\n",
      "Game Frames Survived: 1159\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 45.0\n",
      "Running Reward (last 10 games): 162.8846153846154\n",
      "Explored: 26, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 27\n",
      "EXPLORATION\n",
      "Score: 55.0\n",
      "Reward: 55.0\n",
      "Timesteps: 459\n",
      "Game Frames Survived: 1859\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 55.0\n",
      "Running Reward (last 10 games): 158.88888888888889\n",
      "Explored: 27, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 28\n",
      "EXPLORATION\n",
      "Score: 120.0\n",
      "Reward: 120.0\n",
      "Timesteps: 516\n",
      "Game Frames Survived: 2083\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 120.0\n",
      "Running Reward (last 10 games): 157.5\n",
      "Explored: 28, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 29\n",
      "EXPLORATION\n",
      "Score: 180.0\n",
      "Reward: 180.0\n",
      "Timesteps: 566\n",
      "Game Frames Survived: 2271\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 180.0\n",
      "Running Reward (last 10 games): 158.27586206896552\n",
      "Explored: 29, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 30\n",
      "EXPLORATION\n",
      "Score: 105.0\n",
      "Reward: 105.0\n",
      "Timesteps: 420\n",
      "Game Frames Survived: 1683\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 105.0\n",
      "Running Reward (last 10 games): 156.5\n",
      "Explored: 30, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 31\n",
      "EXPLORATION\n",
      "Score: 110.0\n",
      "Reward: 110.0\n",
      "Timesteps: 357\n",
      "Game Frames Survived: 1451\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 110.0\n",
      "Running Reward (last 10 games): 155.0\n",
      "Explored: 31, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 32\n",
      "EXPLORATION\n",
      "Score: 360.0\n",
      "Reward: 360.0\n",
      "Timesteps: 1127\n",
      "Game Frames Survived: 4509\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 360.0\n",
      "Running Reward (last 10 games): 161.40625\n",
      "Explored: 32, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 33\n",
      "EXPLORATION\n",
      "Score: 140.0\n",
      "Reward: 140.0\n",
      "Timesteps: 551\n",
      "Game Frames Survived: 2203\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 140.0\n",
      "Running Reward (last 10 games): 160.75757575757575\n",
      "Explored: 33, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 34\n",
      "EXPLORATION\n",
      "Score: 120.0\n",
      "Reward: 120.0\n",
      "Timesteps: 492\n",
      "Game Frames Survived: 1973\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 120.0\n",
      "Running Reward (last 10 games): 159.55882352941177\n",
      "Explored: 34, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 35\n",
      "EXPLORATION\n",
      "Score: 455.0\n",
      "Reward: 455.0\n",
      "Timesteps: 690\n",
      "Game Frames Survived: 2767\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 455.0\n",
      "Running Reward (last 10 games): 168.0\n",
      "Explored: 35, Exploited: 0\n",
      "----------\n",
      "----------\n",
      "Game Number: 36\n",
      "EXPLORATION\n",
      "Score: 230.0\n",
      "Reward: 230.0\n",
      "Timesteps: 737\n",
      "Game Frames Survived: 2971\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 230.0\n",
      "Running Reward (last 10 games): 169.72222222222223\n",
      "Explored: 36, Exploited: 0\n",
      "running reward: 169.72 at episode 36, frame count 20000\n",
      "----------\n",
      "----------\n",
      "Game Number: 37\n",
      "EXPLOITATION\n",
      "Score: 95.0\n",
      "Reward: 95.0\n",
      "Timesteps: 508\n",
      "Game Frames Survived: 2053\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 95.0\n",
      "Running Reward (last 10 games): 167.7027027027027\n",
      "Explored: 36, Exploited: 1\n",
      "----------\n",
      "----------\n",
      "Game Number: 38\n",
      "EXPLOITATION\n",
      "Score: 100.0\n",
      "Reward: 100.0\n",
      "Timesteps: 720\n",
      "Game Frames Survived: 2903\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 100.0\n",
      "Running Reward (last 10 games): 165.92105263157896\n",
      "Explored: 36, Exploited: 2\n",
      "----------\n",
      "----------\n",
      "Game Number: 39\n",
      "EXPLOITATION\n",
      "Score: 45.0\n",
      "Reward: 45.0\n",
      "Timesteps: 441\n",
      "Game Frames Survived: 1789\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 45.0\n",
      "Running Reward (last 10 games): 162.82051282051282\n",
      "Explored: 36, Exploited: 3\n",
      "----------\n",
      "----------\n",
      "Game Number: 40\n",
      "EXPLOITATION\n",
      "Score: 65.0\n",
      "Reward: 65.0\n",
      "Timesteps: 425\n",
      "Game Frames Survived: 1707\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 65.0\n",
      "Running Reward (last 10 games): 160.375\n",
      "Explored: 36, Exploited: 4\n",
      "----------\n",
      "----------\n",
      "Game Number: 41\n",
      "EXPLOITATION\n",
      "Score: 60.0\n",
      "Reward: 60.0\n",
      "Timesteps: 455\n",
      "Game Frames Survived: 1831\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 60.0\n",
      "Running Reward (last 10 games): 157.9268292682927\n",
      "Explored: 36, Exploited: 5\n",
      "----------\n",
      "----------\n",
      "Game Number: 42\n",
      "EXPLOITATION\n",
      "Score: 230.0\n",
      "Reward: 230.0\n",
      "Timesteps: 816\n",
      "Game Frames Survived: 3275\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 230.0\n",
      "Running Reward (last 10 games): 159.64285714285714\n",
      "Explored: 36, Exploited: 6\n",
      "----------\n",
      "----------\n",
      "Game Number: 43\n",
      "EXPLOITATION\n",
      "Score: 75.0\n",
      "Reward: 75.0\n",
      "Timesteps: 496\n",
      "Game Frames Survived: 1995\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 75.0\n",
      "Running Reward (last 10 games): 157.67441860465115\n",
      "Explored: 36, Exploited: 7\n",
      "----------\n",
      "----------\n",
      "Game Number: 44\n",
      "EXPLOITATION\n",
      "Score: 550.0\n",
      "Reward: 550.0\n",
      "Timesteps: 907\n",
      "Game Frames Survived: 3637\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 550.0\n",
      "Running Reward (last 10 games): 166.5909090909091\n",
      "Explored: 36, Exploited: 8\n",
      "----------\n",
      "----------\n",
      "Game Number: 45\n",
      "EXPLOITATION\n",
      "Score: 185.0\n",
      "Reward: 185.0\n",
      "Timesteps: 607\n",
      "Game Frames Survived: 2451\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 185.0\n",
      "Running Reward (last 10 games): 167.0\n",
      "Explored: 36, Exploited: 9\n",
      "----------\n",
      "----------\n",
      "Game Number: 46\n",
      "EXPLOITATION\n",
      "Score: 105.0\n",
      "Reward: 105.0\n",
      "Timesteps: 435\n",
      "Game Frames Survived: 1753\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 105.0\n",
      "Running Reward (last 10 games): 165.65217391304347\n",
      "Explored: 36, Exploited: 10\n",
      "----------\n",
      "----------\n",
      "Game Number: 47\n",
      "EXPLOITATION\n",
      "Score: 510.0\n",
      "Reward: 510.0\n",
      "Timesteps: 1271\n",
      "Game Frames Survived: 5105\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 510.0\n",
      "Running Reward (last 10 games): 172.9787234042553\n",
      "Explored: 36, Exploited: 11\n",
      "----------\n",
      "----------\n",
      "Game Number: 48\n",
      "EXPLOITATION\n",
      "Score: 160.0\n",
      "Reward: 160.0\n",
      "Timesteps: 617\n",
      "Game Frames Survived: 2493\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 160.0\n",
      "Running Reward (last 10 games): 172.70833333333334\n",
      "Explored: 36, Exploited: 12\n",
      "----------\n",
      "----------\n",
      "Game Number: 49\n",
      "EXPLOITATION\n",
      "Score: 130.0\n",
      "Reward: 130.0\n",
      "Timesteps: 472\n",
      "Game Frames Survived: 1891\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 130.0\n",
      "Running Reward (last 10 games): 171.83673469387756\n",
      "Explored: 36, Exploited: 13\n",
      "----------\n",
      "----------\n",
      "Game Number: 50\n",
      "EXPLOITATION\n",
      "Score: 350.0\n",
      "Reward: 350.0\n",
      "Timesteps: 795\n",
      "Game Frames Survived: 3181\n",
      "Epsilon: 0.1\n",
      "Running Score (last 10 games): 350.0\n",
      "Running Reward (last 10 games): 175.4\n",
      "Explored: 36, Exploited: 14\n",
      "Solved at episode 50!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate environment\n",
    "env = instantiate_environmnent()\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to make a action\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards\n",
    "# The weights of a target model get updated every 10000 steps thus when the loss between the Q-values is calculated the target Q-value is stable\n",
    "model_target = create_q_model()\n",
    "\n",
    "\n",
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99 # Discount factor for past rewards\n",
    "epsilon = 1.0 # Epsilon greedy parameter\n",
    "epsilon_min = 0.1 # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0 # Maximum epsilon greedy parameter\n",
    "epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32 # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "\n",
    "# Optimizer improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "\n",
    "# Replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "score_history = []\n",
    "\n",
    "# Information variables\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "explored = 0\n",
    "exploited = 0\n",
    "\n",
    "# Number of frames to take random action and observe output and greediness factor\n",
    "epsilon_random_frames = 20000 #50000 # Should change depending on training time\n",
    "epsilon_greedy_frames = 10000 #1000000 # Should change depending on training time\n",
    "\n",
    "# Maximum replay length\n",
    "max_memory_length = 100000\n",
    "\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "\n",
    "while True: # Run until solved\n",
    "    state = np.asarray(env.reset()).reshape(84, 84, 4)\n",
    "    \n",
    "    # Episode information\n",
    "    frames_this_episode = 0\n",
    "    episode_reward = 0\n",
    "    score = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        frames_this_episode += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values from environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            \n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.asarray(state_next).reshape(84, 84, 4)\n",
    "        episode_frame_number = _[\"episode_frame_number\"]\n",
    "        score += reward\n",
    "        \n",
    "        # Reward modifier (This will also affect score)\n",
    "        reward_function(reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(\"----------\")\n",
    "            print(\"----------\")\n",
    "            print(f\"Game Number: {episode_count + 1}\")\n",
    "            if frame_count < epsilon_random_frames:\n",
    "                print(\"EXPLORATION\")\n",
    "                explored += 1\n",
    "            else:\n",
    "                print(\"EXPLOITATION\")\n",
    "                exploited += 1\n",
    "            print(f\"Score: {score}\")\n",
    "            print(f\"Reward: {episode_reward}\")\n",
    "            print(f\"Timesteps: {frames_this_episode}\")\n",
    "            print(f\"Game Frames Survived: {episode_frame_number}\")\n",
    "            print(f\"Epsilon: {epsilon}\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    score_history.append(score)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    running_score = np.mean(score_history[-10:])\n",
    "    \n",
    "    print(f\"Running Score (last 10 games): {running_score}\")\n",
    "    print(f\"Running Reward (last 10 games): {running_reward}\")\n",
    "    print(f\"Explored: {explored}, Exploited: {exploited}\")  \n",
    "    \n",
    "    # Save model checkpoint\n",
    "    #if episode_count % 10 == 0:\n",
    "    #    model.save(\"model.h5\")\n",
    "    \n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count == 50:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "529a1aa7-6a37-40c3-8640-13d65940a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Episode:1 Score:235.0\n",
      "Episode:2 Score:235.0\n",
      "Episode:3 Score:15.0\n",
      "Episode:4 Score:245.0\n",
      "Episode:5 Score:235.0\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n",
    "\n",
    "env = instantiate_environmnent()\n",
    "\n",
    "model = keras.models.load_model(\"model.h5\")\n",
    "video = VideoRecorder(env, \"model.mp4\", enabled=True)\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = np.asarray(env.reset()).reshape(84, 84, 4)\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        batch_state = tf.expand_dims(state, 0)\n",
    "        action = np.argmax(model.predict(batch_state)[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = np.asarray(state).reshape(84, 84, 4)\n",
    "        video.capture_frame()\n",
    "        score += reward\n",
    "    \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71c851-7969-4103-aecd-5177b5469bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
